<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>VMware</title>
    <link rel="icon" href="https://sempernow.github.io/refpages/sa/favicon.png">
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/normalize.css">
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/main.css">
    <!--
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/dev.css">
    -->
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/hljs.github.min.css">
    <style>

    </style>
    <script src="https://sempernow.github.io/refpages/sa/js/hl.min.js"></script>
    <script>hljs.highlightAll()</script>
</head>
<body>
    <main>
        <h1><a href="https://www.vmware.com/" title="VMware.com">VMware</a> | <a href="https://chatgpt.com/share/f5522c3c-a597-42ac-adee-4d445b0836f6" title="ChatGPT.com">Proxmox v. ESXi v. OpenStack</a></h1>

<blockquote>
<p>VMware is now (2023) owned by <strong>Broadcom</strong>,<br>
which has (2024) <a href="https://knowledge.broadcom.com/external/article?legacyId=2107518" title="knowledge.broadcom.com"><strong>discontinued its <em>free</em> ESXi</strong></a> hypervisor&quot;</p>
</blockquote>

<h2><a href="https://en.wikipedia.org/wiki/VMware_ESXi">ESXi</a></h2>

<ul>
<li>Type:<br>
Bare-metal, Type-1 hypervisor.</li>
<li>Purpose:<br>
ESXi is installed directly on server hardware to allow it to run multiple virtual machines (VMs) efficiently by
virtualizing the hardware resources of the physical server.</li>
<li>Use Case:
ESXi is for server-level deployments. It's widely used in data centers and enterprise environments where organizations need to run multiple VMs on a server. ESXi is the foundation of VMware's enterprise-level virtualization stack.</li>
<li>Key Features:<br>
Resource management, high availability, support for clustering, performance tuning, and security for running multiple VMs.</li>
<li>Relation to VMware Workstation and vCenter:<br>
Unlike VMware Workstation, ESXi is for server-level deployments. It can be managed either directly via its web interface or through vCenter for larger environments.</li>
</ul>

<p><strong>Components</strong>/<strong>Features</strong>:</p>

<ul>
<li>Horizon <strong>Connection Server</strong>:
This is the core component of the Horizon VDI architecture. It manages session brokering, authentication, and directs incoming VDI connection requests from users to their respective virtual desktops or published applications.</li>
<li><strong>ESXi Hosts</strong> (Physical Servers):
These are the servers where the VMs or desktops are hosted. They run the VMware ESXi hypervisor, which allows multiple VMs to share physical hardware resources.</li>
<li><strong>Thin Clients</strong>:
These are minimalistic computers that primarily provide network connectivity and the necessary interfaces to interact with the user's virtual desktop. They run thin client operating systems and include client software,
typically the VMware <strong>Horizon Client</strong>.</li>
<li><strong>NSX</strong> (Networking and Security Virtualization)

<ul>
<li>SDN requiring <a href="https://www.vmware.com/products/cloud-infrastructure/vmware-cloud-foundation">VMware Cloud Foundation</a> (<strong>VCF</strong>) : Private Cloud Platform</li>
</ul></li>
<li><strong>vMotion</strong> : live migration of a VM to another physical machine without any downtime

<ul>
<li>Enterprise feature : Requires license</li>
</ul></li>
<li><code>.vmdk</code> : Proprietary virtualization format; <strong>Proxmox</strong> supports that and others (<code>.qcow2</code> and <code>.vdi</code>).</li>
<li><del><a href="https://my.vmware.com/en/web/vmware/evalcenter?p=free-esxi6">Free version</a> for registered users</del></li>
</ul>

<p><strong>Tools</strong>:</p>

<ul>
<li><strong>vSphere Client</strong> is the <strong>administrative interface</strong> for the overall management of VMware's vSphere products, including <strong>ESXi hosts</strong> and <strong>vCenter Servers</strong>.</li>
<li>VMware <strong>Remote Console</strong> (<strong>VMRC</strong>) is used for direct interaction with a VM's operating system, similar to accessing a physical machine's console. It's part of managing VMs within the vSphere environment, but is focused on VM interaction rather than broader infrastructure management.</li>
<li>VMware <strong>Horizon Client</strong> is specifically for accessing virtual desktops and applications in a <strong>VDI</strong> environment.</li>
</ul>

<h2>vCenter</h2>

<ul>
<li>Type:<br>
Centralized management platform for VMware environments.</li>
<li>Purpose:<br>
vCenter allows administrators to manage multiple <strong>ESXi</strong> hosts
and the VMs running on them from a single console.
It provides advanced features like <strong>VM migration</strong> (<strong>vMotion</strong>),
high availability (HA), distributed resource scheduling (DRS), and more.</li>
<li>Use Case:<br>
vCenter is used in enterprise environments where many ESXi hosts and VMs
need to be centrally managed for efficiency, scalability, and high availability.</li>
<li>Key Features:<br>
<strong>Centralized management of multiple ESXi hosts</strong>, resource pooling,
load balancing, VM migration, and performance monitoring.</li>
<li>Relation to ESXi and VMware Workstation:<br>
vCenter is used to manage multiple ESXi hosts,
providing features and functionalities that individual ESXi hosts or VMware Workstation cannot offer on their own. It is not directly related to VMware Workstation, but some users use VMware Workstation for testing and then move to ESXi and vCenter for production.</li>
</ul>

<p>Components/Features:</p>

<ul>
<li>Manage/Access multiple ESXi hosts, all from one console.</li>
<li><strong>vCenter</strong> is deployed on ESXi VM host as <strong>either</strong>:

<ol>
<li><strong>VCSA</strong> (VMware Center (vCenter) Service <strong>Appliance</strong>) (preferred)

<ul>
<li>Was OpenSUSE; now (v6.5+) is VMware <strong>Photon OS</strong></li>
</ul></li>
<li><strong>Windows</strong>, which uses MS SQL database</li>
</ol></li>
<li>Flash (legacy) and HTML5 based interfaces available.</li>
<li>Advanced features; VM Cloning, load balancing, ...</li>
</ul>

<p>The technology that facilitates connection to vCenter and provides the pop-up window
where you interact with the VM is primarily the <strong>VMRC</strong> or the <strong>Web Console</strong>.</p>

<h3>VMware Remote Console (VMRC)</h3>

<p>VMRC connects to vCenter Server, allowing users to interact directly with the guest operating system of a VM through a pop-up window. It handles the keyboard, video, and mouse (<strong>KVM</strong>) streams, making it feel like you are directly logged onto the machine. VMRC can be <strong>launched from the vSphere Web Client or directly as a standalone application</strong>.</p>

<p>Sometimes informally referred to as the &quot;<strong>vSphere Remote Client</strong>&quot;.
The VMRC is actually a KVM-type of utility that provides console access to guest operating systems running on VMware VMs. VMRC allows you to remotely interact with the VM's operating system as if you were sitting in front of it, complete with screen, keyboard, and mouse input.</p>

<h3>vSphere Web Console</h3>

<p>An alternative to VMRC, especially in newer versions of vSphere, is the web console provided directly through the HTML5-based vSphere Client. This console allows for interaction with the VM without needing to install additional software like VMRC.</p>

<p>Both VMRC and the web console use a combination of technologies,
including proprietary VMware protocols, to provide secure and efficient access
to manage and operate the VMs remotely.</p>

<p>These tools ensure that you have near-real-time interaction with the VMs,
enabling tasks such as configuring operating systems, installing applications,
and monitoring operations directly from the console window.</p>

<h2>Guest VM Networking</h2>

<pre><code>[VM1] ----\
[VM2] ----&gt; [vSwitch0] ---[vmnic0]--- [Physical Switch] --- [Router/Gateway]
[VM3] ----/
</code></pre>

<ul>
<li><strong>vSwitch</strong> is a Layer 2 switch

<ul>
<li>It connects VMs to each other (on the same ESXi host).</li>
<li>It connects VMs to physical networks if it has one or more uplinks; physical NICs (vmnic0).</li>
<li>It does not provide Layer 3 routing — no default gateway lives &quot;in&quot; the vSwitch.</li>
</ul></li>
<li><strong>Router/Gateway</strong> is the default gateway for the subnet.

<ul>
<li>If on isolated NAT subnet, this may be merely the NAT device AKA &quot;subnet address&quot;,
whereof ping test is expected to fail.</li>
</ul></li>
</ul>

<h3>Verify Connectivity from a Guest VM</h3>

<p>From inside a host (RHEL) of a <strong>guest VM</strong>,
we can gather useful clues to infer
whether the <strong>vSwitch has an uplink</strong>
and whether the <strong>default gateway is reachable</strong>.</p>

<hr>

<h3>✅ <strong>Check 1: Routing table</strong></h3>

<pre><code class="language-bash">☩ ip route show
default via 192.168.11.1 dev eth0 proto dhcp src 192.168.11.100 metric 100
192.168.11.0/24 dev eth0 proto kernel scope link src 192.168.11.100 metric 100

</code></pre>

<ul>
<li>This shows the <strong>default gateway</strong> (here, <code>192.168.11.1</code>).</li>
<li>If it's missing, the VM won’t be able to route off its subnet.</li>
</ul>

<hr>

<h3>✅ <strong>Check 2: Ping the default gateway</strong></h3>

<pre><code class="language-bash">☩ ping -c 3 -W 1  192.168.11.1
PING 192.168.11.1 (192.168.11.1) 56(84) bytes of data.

--- 192.168.11.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2056ms
</code></pre>

<ul>
<li>If this fails <strong>but other VMs on the same ESXi host are reachable</strong>, as is the case here,
then the vSwitch likely <strong>has no uplink</strong> to the gateway.</li>
</ul>

<p>If on a NAT-isolated subnet, as this host is,
then failure on <code>ping</code> of that NAT device (AKA subnet) address is normal.
Yet we can verify connectivity to an upstream gateway on another subnet:</p>

<pre><code class="language-bash">☩ ping -c 3 -W 1  192.168.28.1
PING 192.168.28.1 (192.168.28.1) 56(84) bytes of data.
64 bytes from 192.168.28.1: icmp_seq=1 ttl=63 time=4.22 ms
64 bytes from 192.168.28.1: icmp_seq=2 ttl=63 time=6.67 ms
64 bytes from 192.168.28.1: icmp_seq=3 ttl=63 time=4.35 ms

--- 192.168.28.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 4.217/5.080/6.672/1.126 ms
</code></pre>

<hr>

<h3>✅ <strong>Check 3: Trace the path to an external host</strong></h3>

<pre><code class="language-bash"># To gateway router of another subnet
☩ traceroute 192.168.28.1
traceroute to 192.168.28.1 (192.168.28.1), 30 hops max, 60 byte packets
 1  _gateway (192.168.11.1)  0.345 ms  0.319 ms  0.304 ms
 2  192.168.28.1 (192.168.28.1)  4.150 ms  4.545 ms  4.108 ms

# To internet host
☩ traceroute 8.8.8.8
traceroute to 8.8.8.8 (8.8.8.8), 30 hops max, 60 byte packets
 1  _gateway (192.168.11.1)  0.515 ms  0.488 ms  0.473 ms
 2  192.168.28.1 (192.168.28.1)  5.500 ms  5.616 ms  5.452 ms
 3  172.27.219.3 (172.27.219.3)  15.673 ms 172.27.219.2 (172.27.219.2)  22.335 ms  19.875 ms
 4  po-53-317-rur201.gambrills.md.bad.comcast.net (68.86.252.33)  19.809 ms po-53-318-rur202.gambrills.md.bad.comcast.net (68.86.252.69)  17.724 ms *
 5  * * *
 6  po-200-xar01.gambrills.md.bad.comcast.net (96.216.84.61)  22.059 ms  26.977 ms  26.948 ms
 7  ae-99-rar01.capitolhghts.md.bad.comcast.net (162.151.61.121)  20.781 ms  25.308 ms  25.283 ms
 8  be-3-arsc1.capitolhghts.md.bad.comcast.net (96.110.235.69)  25.220 ms  16.551 ms  21.570 ms
 9  be-31421-cs02.beaumeade.va.ibone.comcast.net (96.110.40.21)  19.398 ms be-3104-pe04.ashburn.va.ibone.comcast.net (96.110.37.130)  19.331 ms be-3311-pe11.ashburn.va.ibone.comcast.net (96.110.32.130)  17.314 ms
10  * * be-3312-pe12.ashburn.va.ibone.comcast.net (96.110.34.122)  18.597 ms
11  * * *
12  * * dns.google (8.8.8.8)  21.271 ms
</code></pre>

<ul>
<li>If it <strong>hangs immediately</strong>, the VM can’t reach the gateway.</li>
<li>If it reaches the gateway but fails further, your upstream routing or firewall may be blocking.</li>
</ul>

<hr>

<h3>✅ <strong>Check 4: Interface and ARP info</strong></h3>

<pre><code class="language-bash"># Does public interface (here, eth0) have (DHCP) assigned IP address
☩ ip -4 addr show dev eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    inet 192.168.11.100/24 brd 192.168.11.255 scope global dynamic noprefixroute eth0
       valid_lft 200397sec preferred_lft 200397sec

# Is gateway IP in ARP table
☩ ip neigh show
192.168.11.2 dev eth0 lladdr 00:15:5d:1c:2f:01 REACHABLE
192.168.11.1 dev eth0 lladdr 00:15:5d:1c:2f:00 REACHABLE
</code></pre>

<ul>
<li>Confirm the public interface (<code>eth0</code>) has an IP address in the expected subnet.</li>
<li>See whether the gateway IP appears in ARP (if it doesn’t, try pinging it first).

<ul>
<li>Our NAT subnet has no gateway router.</li>
</ul></li>
</ul>

<hr>

<h3>✅ <strong>Optional: Check DNS resolution</strong></h3>

<pre><code class="language-bash">☩ dig google.com

; &lt;&lt;&gt;&gt; DiG 9.16.23-RH &lt;&lt;&gt;&gt; google.com
;; global options: +cmd
;; Got answer:
...
;; ANSWER SECTION:
google.com.             117     IN      A       142.251.163.138
...
;; Query time: 18 msec
;; SERVER: 192.168.11.2#53(192.168.11.2)
;; WHEN: Fri May 09 07:55:10 EDT 2025
;; MSG SIZE  rcvd: 135
</code></pre>

<ul>
<li>If DNS fails but <code>ping 8.8.8.8</code> works, your network might be OK,
but your <code>/etc/resolv.conf</code> is misconfigured.</li>
<li>If both fail, it’s likely a routing or uplink issue.</li>
</ul>

<hr>

<h3>❌ What you <strong>cannot</strong> see from the VM:</h3>

<ul>
<li>Whether the <strong>vSwitch has a physical uplink</strong> (vmnic) attached.</li>
<li>Whether the <strong>port group</strong> is on a trunked VLAN.</li>
<li>The actual <strong>vSwitch configuration</strong> on ESXi.</li>
</ul>

<p>You’ll need access to <strong>vSphere or ESXi host CLI</strong> to see those.</p>

<hr>

<h2>Virtual Desktop Infrastructure (VDI)</h2>

<p>This uses VMware <strong>Horizon</strong>; a VDI platform to provide a virtual desktop and application services, allowing users to access personal desktop environments hosted on VMware ESXi servers through lightweight or thin client devices.</p>

<p>This VDI setup is particularly <strong>popular in enterprise environments</strong> where security, management, and cost efficiency are paramount. It allows organizations to provide a standardized set of applications and resources to users,
regardless of their physical location or the capabilities of their local hardware.</p>

<p>Here's how it generally works:</p>

<p>Prior to authentication, the <strong>client pop-up</strong> in a VMware <strong>Horizon VDI</strong> environment
is typically part of the <strong>Horizon Client</strong> application that's <strong>installed on the thin client</strong>.
This client software is responsible for initiating the connection
to the VMware <strong>Horizon Connection Server</strong>.</p>

<p>Here's how the sequence usually unfolds:</p>

<ul>
<li><strong>Horizon Client Initialization</strong>: When the thin client is powered on and the VMware Horizon Client is launched, the initial screen usually <strong>prompts the user to enter the server address</strong> (the Horizon <strong>Connection Server</strong>) and possibly other connection parameters. This step is necessary for the thin client to know where to direct its authentication request and subsequent virtual desktop session traffic.</li>
<li><strong>Login Screen</strong>: After the server address is provided and successfully reached by the Horizon Client, the next screen typically presented is the login screen. Here, the user is asked to provide their credentials, which might include a username and password. Additional layers of security such as two-factor authentication (2FA) may also be part of this process depending on the organization's security policies.</li>
<li><strong>Session Broker</strong>: Once the credentials are entered and validated by the Connection Server,
it acts as a session broker, determining which resources (virtual desktops or applications)
are available to the user based on their credentials and group memberships.</li>
<li><strong>Resource Selection</strong>: If multiple resources are available (e.g., different virtual desktops or applications),
the user may be presented with a selection screen where they can choose the desired virtual desktop or application to launch.</li>
<li><strong>Connection Establishment</strong>: After selecting the resource, the Horizon Client establishes a connection
using <dfn title="PC-over-IP is a proprietary remote display protocol used in AWS Workspaces and VMware Horizon View"><a href="https://en.wikipedia.org/wiki/Teradici#PCoIP_Protocol"><strong>PCoIP</strong></a></dfn>
or <dfn title="VMware proprietary protocol using that of H.264"><strong>Blast Extreme</strong></dfn> protocols, and the user's virtual desktop session begins. The user then interacts with their desktop environment as if it were running locally,
although it is hosted on the ESXi servers in the data center.</li>
</ul>

<h3>Process</h3>

<ol>
<li><p><strong>User Authentication</strong>: The user logs into the Horizon Client installed on their thin client or another device.
Authentication is usually handled by the Connection Server,
which may integrate with enterprise directory services like Microsoft Active Directory.</p></li>

<li><p><strong>Desktop Provisioning</strong>: Once authenticated, the Connection Server brokers a connection to a virtual desktop instance hosted
on one of the ESXi hosts. This desktop can be a persistent desktop,
which maintains user data and settings between sessions, or a non-persistent desktop, which provides a clean state at every login.</p></li>

<li><p><strong>Connection</strong>: The Horizon Client on the thin client establishes a connection to the virtual desktop using VMware's PCoIP (PC-over-IP)
or Blast Extreme protocols. These protocols are designed to deliver high-performance virtual desktop experiences over the network
with efficient compression and encryption.</p></li>

<li><p><strong>User Session</strong>: The user interacts with the Windows environment as if they were using a local PC,
but all processing happens on the server-side. The thin client simply displays the desktop and handles input and output.</p></li>
</ol>

<h3>Advantages</h3>

<ul>
<li><strong>Security</strong>: Since data is processed and stored in the data center, sensitive information never leaves the secure environment.</li>
<li><strong>Cost-Effective</strong>: Thin clients are generally cheaper to maintain and consume less power than traditional desktop PCs.</li>
<li><strong>Centralized Management</strong>: Updates, patches, and software installations can be managed centrally, which simplifies IT operations.</li>
</ul>

<h2>Workstation</h2>

<ul>
<li>Type:<br>
Desktop virtualization software.</li>
<li>Purpose:<br>
It allows users to run multiple VMs on a single desktop or laptop computer. It's primarily used for personal or small-scale development, testing, and learning.</li>
<li>Use Case:<br>
Ideal for developers, IT professionals, and hobbyists
who want to create and run VMs on their personal machine without the need for dedicated server hardware.</li>
<li>Key Features:<br>
Supports a variety of guest operating systems, snapshots, virtual networking, and resource sharing (like CPU, memory, disk, etc.) between VMs.</li>

<li><p>Relation to ESXi and vCenter:<br>
VMware Workstation <strong>can connect to and manage ESXi hosts</strong>, but it's generally not part of a larger enterprise-grade data center solution.</p></li>

<li><p>VMware Workstation Pro 17.5.2</p>

<ul>
<li><a href="https://1337x.to/torrent/6098934/VMware-Workstation-Pro-17-5-2-23775571-Lifetime-Activation-Serials-AppDoze/">https://1337x.to/torrent/6098934/VMware-Workstation-Pro-17-5-2-23775571-Lifetime-Activation-Serials-AppDoze/</a></li>
</ul></li>
</ul>

<h2>ESXi Subnets and Firewalls</h2>

<p>@ ChatGPT4o</p>

<h3>Q:</h3>

<p>Regarding VMs under ESXi, shouldn't each set of logically related VMs have their own firewalled subnet defined and managed via the hypervisor and perhaps allowing prgrammatic access from the VMs?</p>

<h3>A:</h3>

<p>Yes, it is a good practice to segment logically related VMs into their own firewalled subnets within an ESXi environment.
This approach provides several advantages:</p>

<h3>Advantages of Firewalled Subnets for Logically Related VMs:</h3>

<ol>
<li><p><strong>Security</strong>: By isolating groups of VMs, you can limit the blast radius of any potential security incidents.
If one VM is compromised, the attacker's ability to move laterally within the network is restricted.</p></li>

<li><p><strong>Traffic Control</strong>: Firewalled subnets allow you to control the flow of traffic between different groups of VMs. This ensures that only necessary and approved communication paths are allowed, reducing the risk of unauthorized access.</p></li>

<li><p><strong>Compliance</strong>: Many regulatory frameworks require strict network segmentation to protect sensitive data. Isolating VMs into different subnets can help meet these requirements.</p></li>

<li><p><strong>Performance Management</strong>: Segregating traffic can help in managing and optimizing network performance by reducing congestion and ensuring that critical applications have the necessary bandwidth.</p></li>
</ol>

<h3>Implementation in ESXi:</h3>

<ol>
<li><p><strong>Virtual Switches (vSwitches)</strong>: Use ESXi's virtual switches to create isolated networks for different groups of VMs. Each vSwitch can have its own set of port groups and VLANs.</p>

<ul>
<li>Port groups define the network configuration, including VLAN ID, network traffic policies, and security settings for the connected VMs.</li>
</ul></li>

<li><p><strong>Distributed Virtual Switches (vDS)</strong>: For larger environments, consider using VMware vSphere Distributed Switches, which provide centralized management of network configurations across multiple ESXi hosts.</p></li>

<li><p><strong>Firewall Rules</strong>: Configure firewall rules to control traffic between subnets. This can be done using VMware NSX (Networking and Security Virtualization) if available, or through physical firewalls and network devices if NSX is not in use.</p></li>

<li><p><strong>Programmatic Access</strong>: Allow programmatic access to manage and configure network settings from the VMs. This can be achieved using VMware's APIs and tools like PowerCLI for scripting and automation.</p></li>
</ol>

<h3>Steps to Set Up Firewalled Subnets:</h3>

<blockquote>
<p>Terraform, or Ansible module <code>community.vmware</code>, can be used for this.</p>
</blockquote>

<ol>
<li><strong>Define Subnets</strong>: Plan and define the subnets based on the logical grouping of VMs.</li>
<li><strong>Create vSwitches/vDS</strong>: Set up virtual switches or distributed switches in ESXi to host the subnets.</li>
<li><strong>Assign VLANs</strong>: Assign VLAN IDs to the port groups on the switches to segregate the network traffic.</li>
<li><strong>Configure Firewalls</strong>: Implement firewall rules to control the traffic between different subnets. If using NSX, leverage its micro-segmentation capabilities.</li>
<li><strong>Programmatic Configuration</strong>: Use scripts and automation tools to manage network configurations and ensure consistency across the environment.</li>
</ol>

<h3>Example:</h3>

<ul>
<li><strong>Development Environment</strong>: Subnet A</li>
<li><strong>Production Environment</strong>: Subnet B</li>
<li><strong>Database Servers</strong>: Subnet C</li>
</ul>

<p>Each of these subnets would be isolated on different vSwitches or port groups with appropriate VLANs and firewall rules to control the traffic flow between them, ensuring that development servers cannot directly access production databases, for instance.</p>

<p>This segmentation and isolation strategy is a key part of a robust network security posture in a virtualized environment like ESXi.</p>

<h2><a href="https://chatgpt.com/share/6701397c-d940-8009-b7f7-6d103934228a">Create VMs via IaC method</a></h2>

<h3>1. Terraform / vSphere Provider</h3>

<h3>2. Ansible : <code>community.vmware.vmware_guest</code></h3>

<p>Module to create and manage VMs on vSphere.</p>

<h3>3. vSphere Automation SDK</h3>

<p>VMware provides SDKs for Python, Go, and other languages to automate VM creation and management via API calls.</p>
 
    </main>

    <script src="https://sempernow.github.io/refpages/sa/js/base.js"></script>
    <script>
        ;(function(o, undefined){
            'use strict'
            window.addEventListener('load', () => {
                ;(() => {})//()
                ;(() => {})//()
                ;(() => { // FOO LAB
                    const log = o.log('foo')
                        ,main = o.css('MAIN')
                    log('foo')
                    o.toDOM(main, '<h1>TEST</h1>')
                })//()
            })
        })( (typeof window !== 'undefined') 
            && (window[__APP__] = window[__APP__] || {})
                || (typeof global !== 'undefined') 
                    && (global[__APP__] = global[__APP__] || {})
        );
    </script>
</body>
</html>
