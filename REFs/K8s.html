<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>K8s</title>
    <link rel="icon" href="https://sempernow.github.io/refpages/sa/favicon.png">
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/normalize.css">
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/main.css">
    <!--
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/dev.css">
    -->
    <link rel="stylesheet" href="https://sempernow.github.io/refpages/sa/css/hljs.github.min.css">
    <style>

    </style>
    <script src="https://sempernow.github.io/refpages/sa/js/hl.min.js"></script>
    <script>hljs.highlightAll()</script>
</head>
<body>
    <main>
        <h1><a href="https://kubernetes.io/docs" title="Kubernetes.io">Kubernetes</a> (K8s) | <a href="https://kubernetes.io/releases/">Releases</a></h1>

<h2>TL;DR</h2>

<p>Kubernetes is a universal control plane that is most commonly used to build platforms for managing containerized workloads.</p>

<h2><a href="https://kubernetes.io/docs/home/?path=users&amp;persona=app-developer&amp;level=foundational">Overview</a> | <a href="https://kubernetes.io/docs/reference/tools/" title="kubernetes.io/docs/...">Tools</a> | <a href="https://github.com/kubernetes" title="Kubernetes repo">GitHub</a> | <a href="https://en.wikipedia.org/wiki/Kubernetes">Wikipedia</a></h2>

<ul>
<li>Admin: <a href="K8s.kubeadm.sh"><code>K8s.kubeadm.sh</code></a></li>

<li><p>Client: <a href="K8s.kubectl.sh"><code>K8s.kubectl.sh</code></a></p></li>

<li><p><strong>Pod Network</strong> is the <em>underlay network</em> where actual Pod-to-Pod communication happens. Managed by CNI plugin.</p>

<ul>
<li><p>Cluster-wide <strong>Pod CIDR</strong>:</p>

<pre><code class="language-bash"># Declared upon init: 
$ kubeadm init --pod-network-cidr=10.244.0.0/16
# Declared thereafter (state of etcd):
$ kubectl -n kube-system get cm kubeadm-config -o yaml | grep podSubnet
podSubnet: 10.244.0.0/16
</code></pre></li>

<li><p>Per-Node <strong>CIDRs</strong> managed by CNI are declared at :</p>

<pre><code class="language-bash"># By jsonpath
$ kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'
10.244.0.0/24 10.244.1.0/24 10.244.2.0/24

# By yq 
$ kubectl get nodes -o yaml |yq .items[].spec.podCIDR
10.244.0.0/24
10.244.1.0/24
10.244.2.0/24
</code></pre></li>
</ul></li>

<li><p><strong>Service Network</strong> is a <em>virtual overlay</em> for service discovery.</p>

<ul>
<li><p><strong>CIDR</strong> declared at <code>pod.spec.containers[].command</code><br>
of <strong><code>kube-apiserver</code></strong> : <code>--service-cluster-ip-range</code></p>

<pre><code class="language-bash">$ kubectl -n kube-system get pod -o yaml \
    |grep -- --service-cluster-ip-range
    - --service-cluster-ip-range=10.96.0.0/12
    ... (repeated)
</code></pre></li>

<li><p><strong>Service Discovery</strong></p>

<ul>
<li><p>Tracks Pods belonging to a Service<br>
based on <strong>Label Selectors</strong>.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
name: a-service
spec:
selector:
    app: x  # Matches Pods having this label
</code></pre></li>

<li><p><strong>Dynamically updates</strong> the Service’s <strong>load-balancing rules</strong> as Pods are created, deleted, or rescheduled.</p></li>

<li><p>Kubernetes automatically creates an <strong>Endpoints</strong> (or EndpointSlice) object for the Service. This object holds <strong>the current list of healthy Pod IPs</strong> matching the selector.</p></li>
</ul></li>
</ul></li>
</ul>

<h2>Vanilla Cluster</h2>

<p><a href="https://kubernetes.io/docs/setup/production-environment/">Install a production-environment cluster using <code>kubeadm</code>.</a></p>

<h3>Prep host</h3>

<p>See <a href="K8s.configure-kernel.sh"><code>K8s.configure-kernel.sh</code></a></p>

<h4><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers">cgroup drivers</a> : <code>systemd</code> or <code>cgroupfs</code></h4>

<p>On Linux, control groups constrain resources that are allocated to processes.
The <code>kubelet</code> and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads. There are <strong>two versions</strong> of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new generation of the cgroup API.</p>

<p>Identify the cgroup version on Linux Nodes</p>

<pre><code class="language-bash">stat -fc %T /sys/fs/cgroup/

cgroup ()
{
    fs=$(stat -fc %T /sys/fs/cgroup/);
    [[ $fs == 'tmpfs' ]] &amp;&amp; printf v1 &amp;&amp; return;
    [[ $fs == 'cgroup2fs' ]] &amp;&amp; printf v2 &amp;&amp; return;
    echo unknown
}
</code></pre>

<ul>
<li><code>cgroup2fs</code> is v2; <code>tmpfs</code> is v1.</li>
<li>Hyper-V / AlamLinux8 : v1</li>
<li>Hyper-V / RHEL9 : v2</li>
</ul>

<p><del>If cgroup v1, then set <code>kubelet</code> flag <code>--cgroup-driver</code> to <code>systemd</code>, else set to <code>cgroupfs</code>.</del>
Driver should match the container runtime setting, and if the parent processes are <code>systemd</code>, then should use that.</p>

<h3>Install CRI</h3>

<p>See <a href="K8s.provision-cri.sh"><code>K8s.provision-cri.sh</code></a></p>

<h3>Install K8s tools and CNI Plugins</h3>

<p>See <a href="K8s.provision-kubernetes.sh"><code>K8s.provision-kubernetes.sh</code></a></p>

<h2>Topics of Interest</h2>

<h3><a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Node Allocatable</a> (cgroup)</h3>

<h4>Reserve Compute Resources for System Daemons</h4>

<blockquote>
<p>Kubernetes nodes can be scheduled to Capacity. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node.</p>
</blockquote>

<p>The <code>kubelet</code> exposes a feature named &quot;Node Allocatable&quot; that helps to <strong>reserve compute resources for system daemons</strong>.
Cluster administrators are advised to configure 'Node Allocatable' based on their workload density on each node.</p>

<p>Set at <code>kubelet.service</code> configuration</p>

<pre><code class="language-bash">kubelet --kube-reserved=cpu=500m,memory=500Mi --system-reserved=cpu=500m,memory=500Mi --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;10%
</code></pre>

<p>Set at <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/"><code>KubeletConfiguration</code></a> file</p>

<p>@ <code>/var/lib/kubelet/config.yaml</code></p>

<pre><code class="language-yaml">apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
## DeepSeek advise based on psrss report at node of 2 CPU and 4GiB memory
kubeReserved:
  cpu: &quot;500m&quot;
  memory: &quot;1.5Gi&quot;
  ephemeral-storage: &quot;1Gi&quot;
systemReserved:
  cpu: &quot;500m&quot;
  memory: &quot;1Gi&quot;
  ephemeral-storage: &quot;1Gi&quot;
evictionHard:
  memory.available: &quot;500Mi&quot;
  nodefs.available: &quot;10%&quot;
  # Settings not of DeepSeek
  nodefs.inodesFree: &quot;5%&quot;
  imagefs.available: &quot;15%&quot;
  imagefs.inodesFree: &quot;5%&quot;
...
</code></pre>

<ul>
<li><strong><code>evictionHard</code></strong> : The <code>kubelet</code> will evict Pods under one of the following conditions:

<ul>
<li>When the node's available memory drops below 500MiB.</li>
<li>When the node's main filesystem's available space is less than 10%.</li>
<li>When the image filesystem's available space is less than 15%.</li>
<li>When more than 95% of the node's main filesystem's inodes are in use.</li>
</ul></li>
</ul>

<p>View current settings from the (authenticated) proxy:</p>

<pre><code class="language-bash">kubectl proxy &amp; #=&gt; Starting to serve on 127.0.0.1:8001
curl -sX GET http://localhost:8001/api/v1/nodes/$nodeName/proxy/configz |jq .kubeletconfig.evictionHard

</code></pre>

<p>Settings are applied by default on <code>kubeadm init</code>/<code>join</code>. Modifying this after init requires many kernel-level modifications and systemd unit files and reconfigurations. May be quicker to teardown the cluster and start again.</p>

<pre><code class="language-bash">☩ k get pod -l component=etcd -o yaml |yq '.items[] | [.spec.priorityClassName,.spec.priority]'
- system-node-critical
- 2000001000
- system-node-critical
- 2000001000
- system-node-critical
- 2000001000
</code></pre>

<h3><a href="https://github.com/kubernetes/cloud-provider-vsphere/blob/master/docs/book/cloud_provider_interface.md">CPI (Cloud Provider Interface)</a> | <a href="cpi.png">Graphic</a></h3>

<blockquote>
<p>Unlike all other K8s interfaces (CRI, CNI, CSI), which are runtime interfaces, CPI remains <strong>a buildtime interface</strong>,
allowing only vetted cloud vendors , <code>kubernetes/cloud-provider-$name</code>, into the build.
So, <strong>only members of that cartel</strong> can sell any production-ready K8s <code>Service</code> of type <code>LoadBalancer</code> that is fully (actually) integrated.</p>

<p>MetalLB is a beta toy provided by the cloud cartel to serve as their gatekeeper.</p>
</blockquote>

<p>The CPI is slow-walking what would be the way out of the cartel's grasp.</p>

<ul>
<li><a href="https://chatgpt.com/share/674b1b7d-1eb0-8009-9f42-a46b3f938355">ChatGPT</a></li>

<li><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/">CCM (Cloud Controller Manager)</a> : CCM is a K8s binary (<code>cloud-controller-manager</code>) that handles the CPI; a carving out of the provider-specific code that was in the KCM (Kube Controller Manager) binary (<code>kube-controller-manager</code>).</p>

<ul>
<li>Allows cloud vendors to evolve independently from the core Kubernetes code.</li>
<li>The CCM can be linked to <strong>any cloud provider that satisfies <code>cloudprovider.Interface</code></strong>.</li>
</ul></li>

<li><p><a href="https://cloud-provider-vsphere.sigs.k8s.io/concepts/cpi_overview">Overview of the Cloud Provider Interface</a></p></li>

<li><p><a href="https://github.com/kubernetes/cloud-provider/" title="GitHub"><code>cloud-provider/sample</code></a></p></li>
</ul>

<h4>Prior work</h4>

<ul>
<li><a href="https://kubernetes.io/blog/2023/12/14/cloud-provider-integration-changes/">K8s Cloud Provider Interface (CPI)</a></li>
</ul>

<blockquote>
<p>The Cloud Provider Interface (CPI) is responsible for running all the platform specific control loops that were previously run in core Kubernetes components under Kubernetes Controller Manager (KCM), which is a daemon that embeds the core control loops shipped with Kubernetes. CPI is moved out-of-tree (K8s <code>v1.29+</code>) to allow cloud and infrastructure providers to implement integrations that can be developed, built and released independent of Kubernetes core.</p>
</blockquote>

<ul>
<li><a href="https://github.com/munnerz/keepalived-cloud-provider">keepalived-cloud-provider</a></li>
<li><a href="https://cloud-provider-vsphere.sigs.k8s.io/cloud_provider_interface.html#:~:text=The%20Cloud%20Provider%20Interface%20is%20responsible%20for%20running,developed%2C%20built%20and%20released%20independent%20of%20Kubernetes%20core.">vSphere CPI</a></li>
<li><a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">CCM (Cloud Controller Manager)</a> | <a href="https://k8s-docs.netlify.app/en/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Develop</a>

<ul>
<li><a href="https://www.techtarget.com/searchCloudComputing/tutorial/Get-started-with-Kubernetes-Cloud-Controller-Manager">Getting Started</a></li>
</ul></li>
</ul>

<h3><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels</a> v. <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a></h3>

<pre><code class="language-yaml">...
kind: DaemonSet
metadata:
  annotations:
    deprecated.daemonset.template.generation: &quot;1&quot;
    objectset.rio.cattle.io/applied: H4s...(very long)...AAA
    objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
    objectset.rio.cattle.io/owner-name: traefik
    objectset.rio.cattle.io/owner-namespace: kube-system
  labels:
    objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
    svccontroller.k3s.cattle.io/nodeselector: &quot;false&quot;
    svccontroller.k3s.cattle.io/svcname: traefik
    svccontroller.k3s.cattle.io/svcnamespace: kube-system
  ...
...
</code></pre>

<p>Both are k-v maps <code>[PX/]NAME[=VALUE]</code> used to attach metadata to K8s objects.
Any K8s object may have both, either, or neither.</p>

<ul>
<li><code>PX</code> : <code>0-253</code></li>
<li><code>NAME</code> : <code>1-63</code></li>
<li><code>VALUE</code> : <code>&lt;ANY&gt;</code></li>
</ul>

<p>Both segments must begin and end with an alphanumeric character, <code>[a-zA-Z0-9]</code>,
and may include &quot;<code>-</code>&quot;, &quot;<code>_</code>&quot;, and &quot;<code>.</code>&quot; in between.</p>

<blockquote>
<p>Labels can be <strong>used to select objects</strong> and to find collections of objects that satisfy certain conditions. In contrast, <strong>annotations are not used to identify and select objects</strong>.</p>
</blockquote>

<p>Labels are fixed k-v identifiers utilized by <em>Selectors</em> AKA <em>Label Selectors</em>. For example, (new) Pods of a Service are dynamically discovered and selected by <code>svc.selector.KEY=VALUE</code> (Selector) match with <code>pod.metadata.labels.KEY=VALUE</code> (Label).
Labels are not unique. It is common for a label to be declared at all objects (<code>kind</code>) of an application, e.g., for selection by client:</p>

<pre><code class="language-bash">kubectl get all -l app=cache
</code></pre>

<ul>
<li><code>-l</code>, <code>--selector</code></li>
</ul>

<blockquote>
<p>Mutating the value of an object's label may affect that object and its coupling with other objects, and its relationship with Operators, clients and such. Labels are not immutable, but analogous to the follow-on effects of rebasing a Git project, take care with such mutations.</p>
</blockquote>

<p>Annotations are non-identifying metadata for any and all other information; small or large, structured or unstructured, and <strong>allows characters not permitted by labels</strong>.</p>

<h4>Key patterns</h4>

<p>See &quot;<a href="https://kubernetes.io/docs/reference/labels-annotations-taints/">Well-known Labels, Annotations and Taints</a>&quot; for those <strong>declared dynamically</strong> by IaC methods, e.g., Helm, Operators, Argo CD pipelines, &hellip;.</p>

<ul>
<li><p>Key patterns for apps that are deeply integrated with <code>kubernetes.io</code>:</p>

<pre><code class="language-yaml">annotations:
deployment.kubernetes.io/revision: &quot;1&quot;
meta.helm.sh/release-name: vault
meta.helm.sh/release-namespace: vault  
labels:
app.kubernetes.io/component: store
app.kubernetes.io/instance: vault
app.kubernetes.io/managed-by: helm
app.kubernetes.io/name: vault-agent-injector
</code></pre></li>

<li><p>Key patterns for apps that are scoped to an organizationl unit:</p>

<pre><code class="language-yaml">annotations:
devops.local/owner: team-devordie
devops.local/poc: all@developers.local
devops.local/repo: git.local/devordie/fast-cache
devops.local/description: &quot;Cache for devordie applications.&quot;
labels:
app: fast-cache
ver: v1.0.0
tier: backend
role: cache
env: production
managed-by: argocd.local/devordie/fast-cache
</code></pre>

<pre><code class="language-bash"># Labels : Add as k=v pair : common keys: app, environment, stage, 
kubectl label $kind $name k1=v1
# Labels : Modify
kubectl label $kind $name k1=vZ --overwrite
# Labels : Delete
kubectl label $kind $name k1-
</code></pre>

<pre><code class="language-bash"># Annotation : Add as k=v pair
kubectl annotate $kind $name a/b=c
# Annotation : Modify as k=v pair
kubectl annotate $kind $name a/b=x --overwrite
# View : labels||annotations (either)
kubectl get $kind $name -o jsonpath=&quot;'{.metadata.$either}'&quot;
kubectl get $kind $name -o jsonpath=&quot;'{.metadata.$either.&quot;a/b&quot;}'&quot; #=&gt; 'x'
</code></pre></li>
</ul>

<h4>Q:</h4>

<p>This pattern of an annotation containing the manifest it is annotating is common.
How is such redundancy useful?</p>

<pre><code class="language-bash">☩ k get ds -n kube-system svclb-traefik-fbfbd908 -o yaml \
    |yq '.metadata.annotations.&quot;objectset.rio.cattle.io/applied&quot;' |base64 -d |gzip -d
base64: invalid input
{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;DaemonSet&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;objectset.rio.cattle.io/id&quot;:&quot;&quot;,&quot;objectset.rio.cattle.io/owner-gvk&quot;:&quot;/v1, ...,&quot;numberReady&quot;:0}}
</code></pre>

<h4>A:</h4>

<p>The pattern of storing a resource's manifest (or a portion of it) inside an annotation on the same resource is indeed common in Kubernetes, especially with tools like <strong>Helm</strong>, <strong>Argo CD</strong>, <strong>K3s</strong>, and <strong>Rancher</strong>. While this may seem redundant, <a href="https://chatgpt.com/share/6700260b-b0f8-8009-8a9c-c29f4e56b9d0" title="ChatGPT.com">it serves several important purposes</a>:</p>

<ol>
<li><strong>Tracking State for Reconciliation</strong>

<ul>
<li><strong>Purpose</strong>: Kubernetes operators, controllers, and tools (such as Helm, K3s, and Rio) use this annotation to track the &quot;applied&quot; or &quot;desired&quot; state of a resource. This is important for ensuring that the actual state of the resource matches the intended state.</li>
<li><strong>How It Helps</strong>: By embedding the resource's manifest in the annotation, the tool can always refer back to the exact configuration it applied, without needing to reference an external source (e.g., a Git repository or Helm chart). This allows for automatic reconciliation if the resource drifts from its intended state.</li>
<li><strong>Example</strong>: If a DaemonSet or Deployment is modified manually (outside of the tool's control), the controller can compare the current state to the state stored in the annotation and roll back the changes to maintain the intended configuration.</li>
</ul></li>
<li><strong>Facilitating Rollbacks</strong>

<ul>
<li><strong>Purpose</strong>: The stored manifest serves as a point-in-time snapshot of the resource’s configuration, which can be useful for rollback operations.</li>
<li><strong>How It Helps</strong>: If a change is applied to the resource and causes an issue, the tool can refer back to the manifest in the annotation and restore the resource to that previous known-good state.</li>
<li><strong>Example</strong>: If a deployment is updated but fails to start properly, the system can revert to the previous configuration stored in the annotation.</li>
</ul></li>
<li><strong>Supporting Declarative Systems</strong>

<ul>
<li><strong>Purpose</strong>: Tools like <strong>Helm</strong> and <strong>Argo CD</strong> operate in a declarative manner. They apply a desired state to the cluster and continuously monitor it. By storing the manifest in the annotation, the controller has a reference point for what the desired state should look like.</li>
<li><strong>How It Helps</strong>: The redundancy ensures that the controller doesn't need to consult an external source (like a Git repository or Helm release history) for every reconciliation loop. Instead, it can use the locally stored manifest to reapply the intended state.</li>
</ul></li>
<li><strong>Resource Versioning and Drift Detection</strong>

<ul>
<li><strong>Purpose</strong>: These annotations can store a snapshot of the resource's configuration as it was when last applied, allowing tools to detect if there’s been any drift between the stored configuration and the current live configuration.</li>
<li><strong>How It Helps</strong>: If the live configuration of a resource changes (perhaps due to a manual change or external system), the tool can detect that the live state no longer matches the stored state (i.e., drift has occurred) and can trigger a reconciliation to bring the resource back to its desired state.</li>
<li><strong>Example</strong>: If someone modifies a deployment or service in the cluster directly via <code>kubectl</code>, the system can compare the stored manifest in the annotation and automatically reconcile the difference, effectively self-healing the resource.</li>
</ul></li>
<li><strong>Efficient Local Comparison</strong>

<ul>
<li><strong>Purpose</strong>: Instead of always querying external systems like Helm repositories, Git, or even APIs, storing the manifest locally in the annotation allows tools to operate faster and more efficiently by keeping everything they need locally in the resource metadata.</li>
<li><strong>How It Helps</strong>: For large clusters with many resources, querying an external system for every reconciliation loop could be inefficient. By embedding the manifest, the controller can perform comparisons directly within the cluster's API server, improving performance.</li>
<li><strong>Example</strong>: When Helm performs an upgrade, it can compare the stored manifest in the annotations with the new manifest locally without querying remote resources.</li>
</ul></li>
<li><strong>Audit and Debugging</strong>

<ul>
<li><strong>Purpose</strong>: The stored manifest in the annotation can serve as a historical reference for auditing and debugging purposes, providing a snapshot of the resource at a specific point in time.</li>
<li><strong>How It Helps</strong>: If an issue arises in a cluster, admins can inspect the annotations to understand what configuration was applied last, even if the external state (e.g., in Git or Helm) has been updated or changed.</li>
<li><strong>Example</strong>: In the event of an outage or misconfiguration, an engineer could check the resource's annotations to verify the configuration at the time of failure.</li>
</ul></li>
<li><strong>Consistency in Distributed Systems</strong>

<ul>
<li><strong>Purpose</strong>: In a distributed system like Kubernetes, resources may be managed by multiple components or tools. By storing the applied manifest in the resource’s annotation, every component interacting with the resource can have a consistent view of its configuration.</li>
<li><strong>How It Helps</strong>: This ensures that even if there are multiple controllers or administrators interacting with the resource, everyone can see the same configuration and state, preventing inconsistencies.</li>
</ul></li>
</ol>

<h5>Conclusion:</h5>

<p>Although storing the manifest in an annotation can appear redundant, it provides significant benefits in terms of reconciliation, rollback, drift detection, and auditing. This pattern allows Kubernetes controllers and tools like Helm, K3s, and Argo CD to manage resources in a declarative, self-healing manner, ensuring that the actual state of resources always matches their desired state without requiring constant reference to external sources.</p>

<h3><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">Local ephemeral storage</a> | <a href="https://chatgpt.com/share/2846e45f-b59f-4911-be09-09cd1e4f903c">ChatGPT</a></h3>

<p>The <code>kubelet</code> tracks:</p>

<ul>
<li><code>emptyDir</code> volumes, except volumes of <code>tmpfs</code></li>
<li>Directories holding node-level logs</li>
<li>Writeable container layers</li>
</ul>

<blockquote>
<p>The <code>kubelet</code> tracks <strong><em>only the root filesystem</em></strong> for ephemeral storage.
OS layouts that mount a separate disk to <code>/var/lib/kubelet</code> or <code>/var/lib/containers</code>
<em>will not report ephemeral storage correctly</em>.</p>
</blockquote>

<p>The kubelet writes logs to <code>/var/log</code> by default;
and has a base directory for other locally stored data,
<code>/var/lib/kubelet</code> by default.</p>

<h4>Mounts</h4>

<p><strong>To avoid issues, mount all <code>/var/lib/*</code> on one XFS partition</strong>:</p>

<ul>
<li><code>/var/lib/kubelet</code></li>
<li><code>/var/lib/docker</code></li>
<li><code>/var/lib/containerd</code></li>
</ul>

<p><strong>Mount logs dir on separate partition.</strong></p>

<ul>
<li><code>/var/logs</code></li>
</ul>

<h4>Disk Pressure Thresholds</h4>

<p>@ kubelet configuration:</p>

<pre><code class="language-yaml">evictionHard:
  memory.available: &quot;100Mi&quot;
  nodefs.available: &quot;10%&quot;
  nodefs.inodesFree: &quot;5%&quot;
</code></pre>

<h5>Namespace scoped Quotas/Limits</h5>

<p>@ <code>ResourceQuota</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: my-namespace
spec:
  hard:
    requests.ephemeral-storage: &quot;10Gi&quot; # Total of requests
    limits.ephemeral-storage: &quot;15Gi&quot;   # Total of limits
</code></pre>

<p>@ <code>LimitRange</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limit-range
  namespace: my-namespace
spec:
  limits:
    - type: Container
      default:
        ephemeral-storage: &quot;1Gi&quot;    # Default limit if not specified
      defaultRequest:
        ephemeral-storage: &quot;500Mi&quot;  # Default request if not specified.
      max:
        ephemeral-storage: &quot;2Gi&quot;    # Max allowed
      min:
        ephemeral-storage: &quot;200Mi&quot;  # Min that must be requested
</code></pre>

<h3><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operator (Pattern)</a></h3>

<p>The goal of an Operator is to put operational knowledge into software. Previously this knowledge only resided in the minds of administrators, various combinations of shell scripts or automation software like Ansible. It was outside of your Kubernetes cluster and hard to integrate. With Operators, CoreOS changed that.</p>

<p>Operators implement and automate common Day-1 (installation, configuration, etc.) and Day-2 (re-configuration, update, backup, failover, restore, etc.) activities in a piece of software running inside your Kubernetes cluster, by integrating natively with Kubernetes concepts and APIs. We call this a Kubernetes-native application.</p>

<p>With Operators an application is treated as a single object, and exposes only that
which makes sense for the application to work.</p>

<h3><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/">Manage TLS Certificates</a> | <a href="https://kubernetes.io/docs/tasks/administer-cluster/certificates/">mTLS of Control Plane</a> |</h3>

<pre><code>Organization         K8s Workloads              K8s Control Plane
------------         ---------------------      ------------------------------------------
AD CS : Root CA  -&gt;  Vault : Interm-CA Key  &lt;-  Manually add 
                     Vault : Interm-CA Key  -&gt;  kubelet mTLS CA-signed Certs Create/Rotate
                     Vault : Interm-CA Key  -&gt;  kubelet mTLS CA-signed Certs Create/Rotate
                     Vault : Interm-CA Key  -&gt;  kubectl CSR/approve CA-signed Cert 
                     App CA Trust Bundle    &lt;-  kubectl 
                     App CA Trust Bundle    &lt;-  App (Cert-manager)
</code></pre>

<p>The Intermidary (Subordinate) CA can be scoped in time, capabilities and permitted/excluded domains by declarations in <code>capolicy.inf</code> of the Root CA.</p>

<p>See <a href="https://www.sysadmins.lv/blog-en/x509-name-constraints-certificate-extension-all-you-should-know.aspx">X.509 Name Constraints</a></p>

<h4>Q.</h4>

<p>Is it reasonable for root CA to delegate to an intermediary CA
whose key may be kept in Vault or some such for the purpose of
automating TLS issue/rotation for a K8s  cluster?</p>

<h4>A:</h4>

<p>Yes, delegating a root CA to an intermediary CA whose key is kept in a secure system like HashiCorp Vault is a reasonable and common practice, especially for automating TLS certificate issuance and rotation in Kubernetes or similar environments. Here's why:</p>

<ol>
<li><p>Security and Key Management:</p>

<p>Root CA Security: The root CA is typically kept offline or in a highly secure environment because compromising it would undermine the entire certificate chain. Delegating to an intermediary CA allows you to keep the root key secure while still enabling automated certificate management.
Vault's Security Model: HashiCorp Vault is designed to securely store and manage sensitive data like cryptographic keys. Keeping the intermediary CA's key in Vault allows secure management and restricted access, while enabling automation. Vault can also provide fine-grained access control and logging, enhancing security.</p></li>

<li><p>Automating Certificate Issuance and Rotation:</p>

<p>TLS Certificate Lifecycle: Kubernetes environments often require frequent certificate issuance and renewal, especially for short-lived certificates to enhance security. Automating this process helps avoid certificate expiration and manual intervention, ensuring continuous availability.
Intermediary CA for Automation: By using an intermediary CA whose key is managed in Vault, you can leverage automation tools like cert-manager to handle certificate requests, issuance, and renewal without needing access to the highly sensitive root CA key.</p></li>

<li><p>Flexibility and Scalability:</p>

<p>An intermediary CA allows greater flexibility for handling different certificate policies, lifetimes, and usage scenarios without impacting the root CA. This setup scales well with Kubernetes clusters, which may have a high volume of certificate requests across services and pods.</p></li>

<li><p>Integration with Kubernetes:</p>

<p>Vault Integration: HashiCorp Vault integrates well with Kubernetes through tools like Vault PKI and the Kubernetes secrets engine. This makes it easy to automate certificate issuance directly to services and workloads running in your Kubernetes cluster.
cert-manager: In Kubernetes, cert-manager can work with Vault to automatically issue and renew certificates using the intermediary CA stored in Vault, simplifying the management of TLS for Kubernetes services, ingress, and control plane components.</p></li>

<li><p>Separation of Concerns:</p>

<p>Using an intermediary CA for day-to-day operations (like issuing certificates for the Kubernetes cluster) reduces the risk of exposing the root CA, as the root is only involved when signing the intermediary CA. This separation adds an additional layer of security to your PKI architecture.</p></li>
</ol>

<p>In summary, delegating certificate issuance to an intermediary CA whose private key is stored in Vault is a practical approach to automating TLS issuance and rotation in a Kubernetes environment, ensuring both security and scalability.</p>

<h4>Q:</h4>

<p>What CSR parameters/setting are required to scope the intermediary CA
to some subset of things (domains?) the Root CA would allow?</p>

<h4>A:</h4>

<p>When delegating a root CA to an intermediary CA, certain parameters and settings in the Certificate Signing Request (CSR) are required to ensure that the intermediary CA is scoped correctly and limited in what it can issue. These constraints help ensure the security and integrity of the PKI by limiting the authority of the intermediary CA to a specific subset of domains, usage, or policies as defined by the root CA. The main parameters and settings to consider are:</p>

<ol>
<li><p>Basic Constraints:</p>

<p>CA</p>

<p>: The Basic Constraints extension must be set to indicate that the certificate is for a CA (i.e., it can issue other certificates).
Path Length Constraint: This limits the number of subordinate CAs that can be created beneath the intermediary CA. For example, setting pathLenConstraint=0 means that the intermediary CA can issue end-entity certificates but not delegate authority further to another CA.</p></li>
</ol>

<p>Example:</p>

<pre><code class="language-Makefile">basicConstraints = critical, CA:TRUE, pathlen:0
</code></pre>

<ol>
<li>Key Usage:</li>
</ol>

<p>The Key Usage extension specifies what cryptographic operations the certificate can be used for. For an intermediary CA, the key usage must include:</p>

<pre><code>Certificate Signing: To allow the intermediary CA to sign certificates.
CRL (Certificate Revocation List) Signing: If the intermediary CA will sign its own revocation lists.
</code></pre>

<p>Example:</p>

<pre><code class="language-Makefile">keyUsage = critical, digitalSignature, keyCertSign, cRLSign
</code></pre>

<ol>
<li>Extended Key Usage (Optional):</li>
</ol>

<p>Extended Key Usage (EKU) can be used to further restrict the types of certificates the intermediary CA can issue (e.g., only for TLS server certificates). However, EKU is more commonly applied to end-entity certificates, and it is generally less common in intermediary CA certificates. If you want to restrict the intermediary CA to specific purposes, this can be defined here.</p>

<p>Example:</p>

<pre><code class="language-Makefile">extendedKeyUsage = serverAuth, clientAuth
</code></pre>

<ol>
<li>Name Constraints:</li>
</ol>

<p>The Name Constraints extension restricts the domains for which the intermediary CA can issue certificates. This can be a critical control to limit the authority of the intermediary CA to a specific set of domains or subdomains. For example, the intermediary CA could be restricted to issuing certificates only for example.com and its subdomains.</p>

<p>Example:</p>

<pre><code class="language-Makefile">nameConstraints = critical, permitted;DNS:.example.com
</code></pre>

<pre><code>Permitted Subtrees: Specifies which domains or IP address ranges the intermediary CA is allowed to issue certificates for. For example, `.example.com` would allow issuance for example.com and any subdomain.
Excluded Subtrees: Optionally, you can exclude certain subdomains from the set of allowed domains (e.g., `-example.com`).
</code></pre>

<ol>
<li>Certificate Policies:</li>
</ol>

<p>Certificate Policies allow you to define specific rules or requirements for certificates issued by the intermediary CA. These policies can define practices, legal obligations, or technical constraints.</p>

<pre><code>The root CA can issue the intermediary CA certificate with a specific policy identifier that restricts the types of certificates the intermediary CA can issue.
This policy can also reference an external document, such as a Certificate Practice Statement (CPS), that defines the acceptable usage of the certificate.
</code></pre>

<p>Example:</p>

<pre><code class="language-Makefile">certificatePolicies = 1.3.6.1.4.1.11129.2.5.1
</code></pre>

<ol>
<li>Subject Alternative Name (SAN) and Subject Name:</li>
</ol>

<p>While SAN is more relevant to end-entity certificates (such as those used for TLS), the root CA may impose certain restrictions on the Subject Name or Subject Alternative Names that the intermediary CA can include in the certificates it issues.</p>

<pre><code>Subject: The Subject section should reflect the intermediary CA's identity, typically the organization’s name.
Subject Alternative Name: If desired, you could also scope this to specific domains or IP ranges, although this is more typically enforced through Name Constraints.
</code></pre>

<ol>
<li>CRL Distribution Points (CDP) and Authority Information Access (AIA):</li>
</ol>

<p>These extensions indicate where certificate revocation information and CA certificates can be retrieved. While they don’t directly restrict what the intermediary CA can issue, they are important for ensuring the proper distribution of trust information.</p>

<p>Example:</p>

<pre><code class="language-Makefile">crlDistributionPoints = URI:http://crl.example.com/intermediaryCA.crl
authorityInfoAccess = caIssuers;URI:http://ca.example.com/rootCA.crt
</code></pre>

<ol>
<li>Validity Period:</li>
</ol>

<p>The intermediary CA’s validity period should be scoped appropriately. Typically, it has a shorter validity period than the root CA, ensuring that if compromised, the intermediary CA’s certificates will expire sooner.</p>

<h3><a href="https://kubernetes.io/docs/concepts/security/controlling-access/" title="Kubernetes.io"><strong>Authn</strong>/<strong>Authz</strong></a></h3>

<p>Regarding identity, Kubernetes has
<a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#user-accounts-versus-service-accounts">two categories</a>)
of subject:</p>

<ol>
<li><p><a href="https://kubernetes.io/docs/concepts/security/service-accounts/"><strong><code>ServiceAccount</code></strong></a> :  K8s object for non-human subjects AKA <em>entities</em>; Pod, DaemonSet, CronJob,
Job, CRD, controllers, operators, &hellip;, and <strong>external services</strong> (CI/CD pipelines for example). Upon creation of a Pod, K8s generates/attaches to it a common ServiceAccount named <code>default</code> in Pod's namespace (). This has neither <code>Role</code> nor <code>RoleBinding</code> objects and so its permissions are very limited (depending on cluster policies). Authentication is typically by Bearer Token.</p>

<pre><code class="language-bash">sa=$(k get -n kube-system pod coredns-576bfc4dc7-f9kkh -o jsonpath='{.spec.serviceAccount}')
tkn=$(k create -n kube-system token $sa --duration 10m)
ip=$(k -n kube-system get svc traefik -o jsonpath='{.status.loadBalancer.ingress[].ip}')
# Send GET request to the protected API server
curl -ikH &quot;Authorization: Bearer $tkn&quot; https://$ip:6443/version # JSON response body
</code></pre></li>

<li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes"><strong>user</strong> or <strong>group</strong></a> : K8s concepts of human subjects. Neither are objects of the K8s API; however, both are searched for, and can be authenticated against, by K8s API from client's TLS certificate fields, and mapped to <code>kind</code> of <code>[Cluster]RoleBinding</code> :</p>

<pre><code class="language-bash">$ k explain clusterrolebinding.subjects.kind
GROUP:      rbac.authorization.k8s.io
KIND:       ClusterRoleBinding
VERSION:    v1

FIELD: kind &lt;string&gt;


DESCRIPTION:
    Kind of object being referenced. Values defined by this API group are
    &quot;User&quot;, &quot;Group&quot;, and &quot;ServiceAccount&quot;. If the Authorizer does not recognized
    the kind value, the Authorizer should report an error.
</code></pre>

<ul>
<li><p>Organization (<code>O</code>) maps to <code>Group</code> and Common Name (<code>CN</code>) fields of client's TLS certificate.
For example, &quot;<code>Subject: O = team-x1-devs + O = system:basic-user, CN = fred</code>.
To see that of <code>default</code> user of a default kubeconfig:</p>

<pre><code class="language-bash"># View certificate text
kubectl config view --raw -o jsonpath='{.users[].user.client-certificate-data}' \
|base64 -d \
|openssl x509 -text -noout

# Send GET request to the protected API server using TLS certificate and key
curl -k \
--cert &lt;(k config view --raw -o jsonpath='{.users[0].user.client-certificate-data}' |base64 -d) \
--key &lt;(k config view --raw -o jsonpath='{.users[0].user.client-key-data}' |base64 -d) \
https://$(k -n kube-system get svc traefik -o jsonpath='{.status.loadBalancer.ingress[].ip}'):6443/version
</code></pre></li>

<li><p>The build-in <strong><code>system:masters</code></strong> group is the break-glass <em>uber admin</em> having unrestricted access to K8s API;
this subject is typically bound to the <code>cluster-admin</code> ClusterRole, <strong>allowing any action</strong> (<code>* verb</code>)
on <strong>any resource</strong> (<code>* resource</code>) in <strong>any API group</strong> across <strong>all namespaces</strong>.</p>

<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
...
name: cluster-admin
...
subjects:
- apiGroup: rbac.authorization.k8s.io
kind: Group
name: system:masters
</code></pre></li>

<li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">Create TLS certificate</a> for a <strong>user</strong> or <strong>group</strong> : See <a href="K8s.users.and.groups.sh"><code>K8s.users.and.groups.sh</code></a> .</p></li>
</ul></li>

<li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/" title="Kubernetes.io"><strong>Authentication</strong></a> (Authn)</p>

<ul>
<li>Two (Data-plane) scenarios

<ol>
<li>Clients authenticating against the K8s API server

<ul>
<li>The two most common methods:

<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user" title="Kubernetes.io">X.509 certificate issued by K8s CA</a></li>
<li>Token (JWTs) generated by an OIDC provider, e.g., <strong>Dex</strong> or <strong>Keycloak</strong>, that acts as proxy of upstream Identity Provider (<strong>IdP</strong>), such as AD/LDAP, against which it authenticates a subject, which is <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#request-attributes-used-in-authorization" title="Kubernetes.io">presumably recognizable to K8s</a>, i.e., a user/group or <code>ServiceAccount</code> having K8s <code>cluster.user</code> and (<code>Cluster</code>)<code>RoleBinding</code>.</li>
</ul></li>
</ul></li>
<li>Users authenticating at web UI against an application running on the cluster.

<ul>
<li>Token (JWTs) generated by an OIDC provider (same as above method).</li>
</ul></li>
</ol></li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authentication-strategies" title="Kubernetes.io">Authentication Plugins</a>

<ul>
<li>Static Token file

<ul>
<li>Bearer token</li>
<li>Service Account token</li>
</ul></li>
<li>X.509 certificates</li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" title="Kubernetes.io">Open ID Connect (OIDC) token</a></li>
<li>Authentication proxy</li>
<li>Webhook</li>
</ul></li>
</ul></li>

<li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/" title="Kubernetes.io"><strong>Authorization</strong></a> (Authz) | Modules/<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules" title="Kubernetes.io">Modes</a><br>
Regardless of authentication method,
K8s can implement Role-based Access Control (<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" title="Kubernetes.io">RBAC</a>) model
against subjects (<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#request-attributes-used-in-authorization" title="Kubernetes.io">known by request attribute(s)</a>)
using a pair of K8s objects for each of the two scopes of K8s API resources (<code>api-resources</code>):</p>

<ol>
<li>Namespaced (<code>Deployment</code>, <code>Pod</code>, <code>Service</code>, &hellip;)

<ul>
<li><code>Role</code> : Rules declaring the allowed actions (<code>verbs</code>) upon <code>resources</code> scoped to APIs (<code>apiGroup</code>).</li>
<li><code>RoleBinding</code> : Binding a subject (authenticated user or ServiceAccount) to a role.</li>
</ul></li>
<li>Cluster-wide (<code>PersistentVolume</code>, <code>StorageClass</code>, &hellip;)

<ul>
<li><code>ClusterRole</code></li>
<li><code>ClusterRoleBinding</code></li>
</ul></li>
</ol></li>
</ol>

<h3><code>kubectl</code></h3>

<p>Client CLI to communicate with Kubernetes API server
Must be configured to cluster, context and have user of <code>kubeadm</code>'s config.</p>

<h3><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig</a> file</h3>

<blockquote>
<p>A reference to any file that is used to configure access to clusters.
This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</p>
</blockquote>

<p>Optionally set using K8s environment variable: <code>KUBECONFIG=/path/to/that/file</code></p>

<p>Example @ <code>~/.kube/config</code> (default location)</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Config
preferences: {}
current-context: kubernetes-admin@kubernetes
clusters:
- cluster:
    server: https://192.168.0.81:6443
    certificate-authority-data: REDACTED    # CA certificate        (public)
    # CA Certificate : /etc/kubernetes/pki/ca.crt
    # CA Key         : /etc/kubernetes/pki/ca.key
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
    namespace: default
  name: kubernetes-admin@kubernetes
users:
- name: kubernetes-admin
  user
    client-key-data: REDACTED               # Client key            (private)
    client-certificate-data: REDACTED       # Client certificate    (public)
</code></pre>

<ul>
<li>This kubeconfig may be clone of server config (only of a control node).

<ul>
<li><code>/etc/kubernetes/admin.conf</code>.</li>
</ul></li>
<li>This client (<code>kubectl</code>) configuration file AKA kubeconfig
holds <code>clusters</code>, <code>contexts</code>, and <code>users</code>.</li>

<li><p>The <strong>client</strong> is <strong>configured to the <code>current-context</code></strong>
by that declared <code>contexts[].name</code> value, having form <code>USER@CLUSTER</code>,
which is <code>&lt;context.user&gt;@&lt;context.cluster&gt;</code>.</p>

<pre><code class="language-bash"># Get the current context 
kubectl config current-context #=&gt; kubernetes-admin@kubernetes
</code></pre>

<ul>
<li>A <code>context</code> has <code>contexts[].name</code>, and is defined by

<ul>
<li><code>cluster</code></li>
<li><code>user</code></li>
<li><code>namespace</code></li>
<li>A <code>cluster</code> has <code>clusters[].name</code>, and is defined by

<ul>
<li><code>server: https://&lt;ip-address&gt;:&lt;port&gt;</code></li>
<li><code>certificate-authority-data: &lt;ca-certificate&gt;</code>

<ul>
<li>The public element of CA's key-cert pair.</li>
</ul></li>
</ul></li>
<li>A <code>user</code> has <code>users[].name</code>, and is defined by it key-cert pair

<ul>
<li><code>client-key-data: &lt;private-key&gt;</code></li>
<li><code>client-certificate-data: &lt;public-certificate&gt;</code></li>
</ul></li>
<li>A <code>namespace</code> is just a <code>&lt;string&gt;</code>.</li>
</ul></li>
</ul></li>
</ul>

<h4>Select a <code>context</code> (<code>current-context</code>)</h4>

<pre><code class="language-bash">contexts_name=${user}@$cluster # contexts[].name: &lt;context.user&gt;@&lt;context.cluster&gt;
kubectl config use-context $contexts_name 
</code></pre>

<h4>kubeconfig : <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts">Configure Access to Multiple Clusters</a></h4>

<p>Add/Modify context(s), cluster(s), and/or credentials (user(s))</p>

<pre><code class="language-bash">kubectl config --kubeconfig=$file set-context $contexts_name ...
kubectl config --kubeconfig=$file set-cluster $clusters_name ...
kubectl config --kubeconfig=$file set-credentials $users_name ...
</code></pre>

<p>Create and set the target kubeconfig (template)</p>

<p>@ <code>/tmp/config-demo</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
  name: development
- cluster:
  name: test

users:
- name: developer
- name: experimenter

contexts:
- context:
  name: dev-frontend
- context:
  name: dev-storage
- context:
  name: exp-test
</code></pre>

<p>Modify that kubeconfig</p>

<pre><code class="language-bash">file='/tmp/config-demo'

# Add CLUSTER details

clusters_name='development'
server='https://1.2.3.4'
kubectl config --kubeconfig=$file set-cluster $clusters_name \
    --server=$server \
    --certificate-authority=$ca_cert

clusters_name='test'
server='https://5.6.7.8'
kubectl config --kubeconfig=$file set-cluster $clusters_name \
    --server=$server \
    --insecure-skip-tls-verify

# Add USER details

users_name='developer'    # users[].name NOT user.username 
kubectl config --kubeconfig=$file set-credentials $users_name \
    --client-certificate=&quot;$cert&quot; \
    --client-key=&quot;$key&quot;

users_name='experimenter' # users[].name
username='exp'            # users[].name.user.username
password='abc123'
kubectl config --kubeconfig=$file set-credentials $users_name \
    --username=$username \
    --password=$password  # BAD PRACTICE

# Add CONTEXT details

contexts_name='dev-frontend'
clusters_name='development'
ns='frontend'
users_name='developer'    
kubectl config --kubeconfig=$file set-context $contexts_name \
    --cluster=$clusters_name \
    --namespace=$ns \
    --user=$users_name

contexts_name='dev-storage'
ns='storage'
kubectl config --kubeconfig=$file set-context $contexts_name \
    --cluster=$clusters_name \
    --namespace=$ns \
    --user=$users_name

contexts_name='exp-text'
clusters_name='test'
ns='default'
users_name='experimenter' 
kubectl config --kubeconfig=$file set-context $contexts_name \
    --cluster=$clusters_name \
    --namespace=$ns \
    --user=$users_name

</code></pre>

<ul>
<li><code>user.{username,password}</code>
exist only for normal users,
not for ServiceAccounts.</li>
<li>Use <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins"><code>client-go</code> credential plugins</a></li>
</ul>

<h3><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Admission Controller</a></h3>

<p>An admission controller is code of <code>kube-apiserver</code> that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p>

<p>Admission controllers may be validating, mutating, or both. <strong>Mutating controllers may modify objects</strong> related to the requests they admit; <strong>validating controllers may not</strong>.</p>

<p>Admission controllers <strong>limit requests to create, delete, modify</strong> objects. Admission controllers can also block custom verbs, such as a request connect to a Pod via an API server proxy. Admission controllers do not (and cannot) block requests to read (get, watch or list) objects.</p>

<pre><code class="language-bash"># List all enabled admission controllers
kube-apiserver -h |grep enable-admission-plugins

# Enable some admission controllers
kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...

# Disable some 
kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...
</code></pre>

<h3>Pods Inherit Environment</h3>

<p>All Pods of a Namespace inherit the environment variables created by certain mechanisms:</p>

<p>The common environment variables like <code>KUBERNETES_*</code> come from Kubernetes itself and are automatically injected into every container running in a pod. These variables are part of Kubernetes' internal mechanism to provide containers with necessary information about the cluster environment.</p>

<h4>Key Sources of Common Environment Variables:</h4>

<ol>
<li><p><strong>Service Environment Variables</strong>: Kubernetes automatically creates environment variables for each service <strong>in the same namespace</strong> as the pod, providing the pod with the necessary information to interact with the services in the cluster. Note the Pod/container port is not included. These variables follow a pattern:</p>

<ul>
<li><code>${SVC_NAME}_SERVICE_HOST</code> : Service IP address.</li>
<li><code>${SVC_NAME}_SERVICE_PORT</code> : Service port exposed.</li>
<li><code>${SVC_NAME}_PORT</code> : Service's full TCP URL, including protocol and port.</li>
<li>...</li>

<li><p>Example:</p>

<ul>
<li><p><strong>Service</strong>:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
name: my-service
spec:
selector:
app: my-app
ports:
- protocol: TCP
  port: 80          # Service port
  targetPort: 8080  # Pod/Container port
</code></pre></li>

<li><p><strong>Environment</strong> injected into <strong>newer Pods</strong>:</p>

<ul>
<li><code>MY_SERVICE_SERVICE_HOST=10.96.0.2</code></li>
<li><code>MY_SERVICE_SERVICE_PORT=80</code></li>
<li><code>MY_SERVICE_PORT=tcp://10.96.0.2:80</code>

<ul>
<li>Note this is <em>not</em> an IANA scheme, but rather a convention adopted by projects (Docker, Redis, ZeroMQ, ...) to distinguish non-HTTP TCP from other transports (UDP, FTP, ) of Layer 4.</li>
</ul></li>
</ul></li>
</ul></li>

<li><p>DNS-Based Service Discovery (Alternative): In addition to environment variables, Kubernetes provides DNS-based service discovery, where services can be accessed via their DNS names like <code>${SVC_NAME}.default.svc.cluster.local</code>. This is often preferred over relying on environment variables because it provides more flexibility and reduces reliance on environment variable injection. This behavior is the default configuration, so services are discovered by <strong>pre-existing Pods</strong>.</p></li>
</ul></li>

<li><p><strong>Pod and Container Metadata</strong>:</p>

<ul>
<li>Variables like <code>HOSTNAME</code> are set to the name of the pod, and <code>HOME</code> is set to the home directory of the container's root user.</li>
<li>These variables help containers know their own identity and environment.</li>
</ul></li>

<li><p><strong>Default Shell Environment</strong>:</p>

<ul>
<li>Variables like <code>PATH</code>, <code>TERM</code>, and <code>HOME</code> are standard environment variables that are typically provided by the container's base image or operating system. These are not injected by Kubernetes but come from the container's environment setup.</li>
</ul></li>

<li><p><strong>ConfigMaps</strong>:</p>

<ul>
<li><strong>Purpose</strong>: ConfigMaps are used to store configuration data as key-value pairs. Pods can reference ConfigMaps to set environment variables.</li>
<li><strong>Effect</strong>: If multiple pods within a namespace reference the same ConfigMap in their environment configuration, they will share the same environment variables.</li>

<li><p><strong>Example</strong>:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
name: my-config
data:
MY_VAR: &quot;my_value&quot;
</code></pre>

<p>Pods can use this ConfigMap to set an environment variable:</p>

<pre><code class="language-yaml">envFrom:
- configMapRef:
 name: my-config
</code></pre></li>
</ul></li>

<li><p><strong>Secrets</strong>:</p>

<ul>
<li><strong>Purpose</strong>: Secrets store sensitive data like passwords, tokens, or keys. Similar to ConfigMaps, Secrets can be used to set environment variables in pods.</li>
<li><strong>Effect</strong>: Pods that reference the same Secret will share the environment variables derived from that Secret.</li>

<li><p><strong>Example</strong>:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
name: my-secret
data:
MY_SECRET: bXktdmFsdWU=
</code></pre>

<p>Pods can use this Secret to set an environment variable:</p>

<pre><code class="language-yaml">envFrom:
- secretRef:
 name: my-secret
</code></pre></li>
</ul></li>

<li><p><strong>ServiceAccounts</strong>:</p>

<ul>
<li><strong>Purpose</strong>: ServiceAccounts provide identity to pods, allowing them to authenticate to the Kubernetes API server.</li>
<li><strong>Effect</strong>: Pods using the same ServiceAccount will inherit environment variables related to service authentication, such as <code>KUBERNETES_SERVICE_HOST</code>, <code>KUBERNETES_SERVICE_PORT</code>, and the token associated with the ServiceAccount.</li>
</ul></li>

<li><p><strong>Downward API</strong>:</p>

<ul>
<li><strong>Purpose</strong>: The Downward API allows pods to access metadata about themselves or their environment, such as the pod's name, namespace, labels, and annotations.</li>
<li><strong>Effect</strong>: If multiple pods in a namespace are configured to use the Downward API, they can inherit environment variables related to their own metadata.</li>

<li><p><strong>Example</strong>:</p>

<pre><code class="language-yaml">env:
- name: MY_POD_NAME
valueFrom:
 fieldRef:
   fieldPath: metadata.name
</code></pre></li>
</ul></li>

<li><p><strong>NetworkPolicies</strong>:</p>

<ul>
<li><strong>Purpose</strong>: Although not directly related to environment variables, NetworkPolicies can indirectly cause pods to be aware of certain configurations or restrictions in their environment.</li>
</ul></li>

<li><p><strong>Admission Controllers and Mutating Webhooks</strong>:</p>

<ul>
<li><strong>Purpose</strong>: Admission controllers or mutating webhooks can modify the pod spec before it is created, including adding environment variables.</li>
<li><strong>Effect</strong>: If a webhook is set up to inject environment variables based on specific criteria (e.g., all pods in a namespace), those pods will inherit the same environment variables.</li>
</ul></li>

<li><p><strong>PodSecurityPolicies (PSPs)</strong>:</p>

<ul>
<li><strong>Purpose</strong>: While PSPs are used to enforce security standards, they might influence environment variables if they enforce certain runtime configurations.</li>
</ul></li>

<li><p><strong>Custom Resources</strong>:</p>

<ul>
<li><strong>Purpose</strong>: Custom resources and operators can be designed to inject environment variables into pods based on custom logic.</li>
<li><strong>Effect</strong>: If deployed in a namespace, these can cause all pods to inherit specific environment variables based on custom policies.</li>
</ul></li>
</ol>

<h4>How These Variables Are Injected:</h4>

<ul>
<li><p><strong>Kubernetes Service Discovery Mechanism</strong>:</p>

<ul>
<li>When a service is created in Kubernetes, it automatically injects environment variables into all pods within the same namespace. This is part of Kubernetes' built-in service discovery mechanism.</li>
<li>These variables are made available by the kubelet when it starts the container in a pod.</li>
</ul></li>

<li><p><strong>Pod Specification</strong>:</p>

<ul>
<li>Some environment variables can be explicitly defined in the pod specification (e.g., in a <code>Deployment</code>, <code>DaemonSet</code>, or when using <code>kubectl run</code>), but the ones you are seeing are automatically injected by Kubernetes.</li>
</ul></li>
</ul>

<p>In most cases, the inheritance or sharing of environment variables is due to explicit configuration via objects like ConfigMaps, Secrets, or ServiceAccounts, or due to cluster-wide policies applied by admission controllers or other custom mechanisms.</p>

<p>These common environment variables provide a way for containers to discover and connect to other services within the Kubernetes cluster, facilitating communication and integration between components.</p>

<h3><a href="https://kubernetes.io/docs/concepts/workloads/pods/downward-api/">Downward API</a> Example</h3>

<p>Used to configure a Pod's environment variables to expose information about itself to containers running in the Pod.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
...
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ &quot;sh&quot;, &quot;-c&quot;]
      args:
      - while true; do
          echo -en '\n';
          printenv MY_NODE_NAME MY_POD_NAME MY_POD_IP;
          sleep 10;
        done;
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
</code></pre>

<pre><code class="language-yaml">apiVersion: batch/v1
kind: Job
spec:
  template:
    metadata:
      labels:
        ...
        app.kubernetes.io/name: ingress-nginx
        ...
    spec:
      containers:
      - args:
        - create
        ...
        - --namespace=$(POD_NAMESPACE)
        ...
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace

apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  containers:
  - name: example-container
    image: busybox
    env:
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
    args:
    - |
      echo &quot;If by Kubernetes substitution: $(POD_NAMESPACE)&quot;; \
      echo &quot;If from shell environment: ${POD_NAMESPACE}&quot;
</code></pre>

<h3>Naming convention</h3>

<p><strong>Q:</strong> Kubernetes is written in Golang. And best practices of Golang include naming convention that names should not &quot;stutter&quot;. For example, if a struct value should not include the struct key name. However, it seems that Kubernetes has adopted the exact opposite convention. For example pod.volume.name=this-volume is the typical namiing convention. What's going on here? Is that just bad practices proliferating, or is this intentional?</p>

<p><strong>A:</strong> The naming patterns you've observed, such as the redundancy in Kubernetes object names (e.g., appending <code>-service</code> to service names), highlight an interesting intersection between Kubernetes' declarative, self-describing architecture and the practical aspects of managing resources in a complex system. This redundancy, while seemingly at odds with the principles of naming clarity and efficiency, serves several purposes in the Kubernetes ecosystem:</p>

<ol>
<li><p><strong>Contextual Clarity</strong>: In large, complex deployments, where dozens or even hundreds of services, pods, and other resources interact, the additional context provided by such suffixes (<code>-service</code>, <code>-pod</code>, etc.) can be invaluable for quick identification and differentiation of resources. This is particularly true when resources of different types share similar base names, or when viewing aggregated logs or metrics across the system.</p></li>

<li><p><strong>Convention and Consistency</strong>: Kubernetes has spawned its own set of conventions and best practices, developed by the community and the experiences of its users. These conventions, including naming patterns, help maintain consistency across different deployments and projects. For new users, learning these patterns can make it easier to understand and navigate Kubernetes resources.</p></li>

<li><p><strong>Automation and Tooling Compatibility</strong>: Many tools and scripts are built around these naming conventions, expecting certain patterns to be present in resource names. This can help in automating tasks such as monitoring, logging, or CI/CD processes, where being able to predict and parse resource names based on their suffixes or prefixes becomes a powerful feature.</p></li>

<li><p><strong>Namespacing and Scope</strong>: Kubernetes allows for complex scoping and namespacing of resources. The redundant naming conventions can help in scoping resources appropriately, making it clearer which resources belong to which domain or functional area within a larger system.</p></li>

<li><p><strong>Human Factors</strong>: Ultimately, Kubernetes is used by humans, and the naming conventions reflect a balance between machine efficiency and human readability. While the redundancy might be &quot;noise&quot; in a strictly informational sense, it aids in quick comprehension and reduces cognitive load when scanning through resource lists or configurations.</p></li>
</ol>

<p>Despite these reasons, it's important for users and teams to develop and follow their own conventions that best suit their workflows and organizational practices. Kubernetes is flexible enough to accommodate different naming strategies, and what might be redundant or noisy in one context could be clarity-enhancing in another. The key is finding the right balance between Kubernetes' declarative nature, the practical requirements of managing complex systems, and the preferences and practices of the teams involved.</p>

<h3><a href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/">Static Pods</a></h3>

<p><strong>Q:</strong> Why does <code>kubeadm init --pod-network-cidr=&quot;10.100.0.0/12&quot;</code> assign each Static Pod (of the K8s-core Pods) an IP address in the node-network (host) CIDR versus that of the explicitly-declared Pod Network?</p>

<p><strong>A:</strong> Static Pods are a special type of pods in Kubernetes that are <strong>managed directly by the <code>kubelet</code> on a specific node</strong>. Unlike regular pods, which are part of a deployment or replica set, static pods are defined as YAML files on the node itself. <strong>They do not reside in the etcd datastore</strong> like other pods.</p>

<p>The reason static pods are assigned IP addresses on the node network (host network) rather than the explicitly-declared pod network CIDR is due to their unique nature:</p>

<ul>
<li><p>Local Management:<br>
Static pods are managed locally by the <code>kubelet</code> running on the node.
<strong>They do not go through the Kubernetes control plane</strong> (API server, controller manager, scheduler).
As a result, they are not subject to the same network overlay and pod networking rules as regular pods.
The <code>kubelet</code> on each node is responsible for managing Static Pods. It directly interacts with the container runtime (such as Docker or containerd) to start and stop containers for these pods. Since the <code>kubelet</code> is responsible for managing these pods on the node level, it assigns IP addresses to them based on the node's network.</p></li>

<li><p>Early Bootstrapping:<br>
Static pods are created during the early bootstrapping phase of the <code>kubelet</code>.
At this point, the full Kubernetes control plane (including the network overlay) may not be fully operational.
Assigning an IP address from the node network ensures that static pods can communicate with each other and with other local services (such as the <code>kubelet</code> itself) immediately after boot.</p></li>

<li><p>Minimal Dependencies:<br>
Static pods have minimal dependencies.
They rely only on the <code>kubelet</code> and the local Docker runtime (or other container runtime).
By using the node network, they avoid any potential issues related to network plugins or overlay networks.</p>

<ul>
<li>No involvement of <code>kube-proxy</code>: Regular Pods created through the API server go through <code>kube-proxy</code>, which sets up the necessary network rules to enable communication between Pods across the cluster. Static Pods, being managed by the <code>kubelet</code>, don't go through <code>kube-proxy</code>, and their networking is more direct.</li>
</ul></li>

<li><p>No Network Abstraction:<br>
Unlike regular pods, which are part of a logical network abstraction,
static pods are tightly coupled to the node.
Their IP addresses are directly associated with the node’s network interfaces.
This simplicity allows them to function reliably
even in scenarios where the full Kubernetes networking stack is not yet available.</p></li>
</ul>

<p>In summary, static pods serve as a bridge between the host system and the Kubernetes world. Their direct association with the node’s network ensures their availability and functionality during the early stages of cluster bootstrapping. However, it’s essential to understand that static pods are an exception, and regular pods follow the rules defined by the pod network overlay (such as Calico, Flannel, or others) once the cluster is fully operational .</p>

<p><strong>Q:</strong> Why declare <code>10.244.0.0/12</code> as Kubernetes default Pod Network CIDR when it is exactly the same as <code>10.240.0.0/12</code>, the latter of which reveals the first IP address of that declared range?</p>

<p><strong>A:</strong> The choice of <code>10.244.0.0/12</code> as the default Pod Network CIDR in Kubernetes is indeed interesting. Let’s explore why this seemingly redundant range was selected:</p>

<ul>
<li><p>Historical Context:<br>
The <code>10.244.0.0/12</code> range was chosen early in Kubernetes development.
At that time, the decision might not have been as deliberate as it appears now.
It’s possible that the choice was made without considering the similarity to <code>10.240.0.0/12</code>.</p></li>

<li><p>Avoiding Commonly Used Ranges:<br>
Kubernetes needed a private IP address range for pod networking.
The <code>10.0.0.0/8</code> address space is commonly used for private networks.
To avoid conflicts with existing networks, Kubernetes opted for a less common range.</p></li>

<li><p>Granularity and Address Space:<br>
The <code>/12</code> subnet provides a large address space (<code>4096</code> addresses).
Kubernetes clusters can scale significantly, and having ample IP addresses is essential.
The granularity allows for efficient allocation of pod IPs.</p></li>

<li><p>Consistency and Predictability:<br>
Kubernetes aims for consistency across clusters.
By using a specific range like <code>10.244.0.0/12</code>,
administrators can predict the IP addresses assigned to pods.
This consistency simplifies network management and troubleshooting.</p></li>

<li><p>Avoiding Ambiguity:<br>
The choice of <code>10.244.0.0/12</code> avoids ambiguity.
If <code>10.240.0.0/12</code> were used, the first IP address (10.240.0.1)
might be mistaken for a special purpose (such as a gateway or DNS server).
By starting at <code>10.244.0.1</code>, Kubernetes ensures that the entire range is available for pod IPs.</p></li>
</ul>

<p>In summary, while the similarity between <code>10.244.0.0/12</code> and <code>10.240.0.0/12</code> might raise eyebrows, the decision likely prioritized consistency, predictability, and avoiding common address spaces. Kubernetes architects aimed for a balance between practicality and uniqueness when defining the default Pod Network CIDR.</p>

<h3><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a></h3>

<p>Exposes a Deployment to a port under a protocol, sans IP Address(es).
E.g., Create <code>my-service</code> to target TCP port <code>9376</code> on any Pod having the label <code>app.kubernetes.ip/name: MyApp</code> :</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
</code></pre>

<h3>Kubernetes API Server (Master)</h3>

<h4><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/"><code>kube-apiserver</code></a></h4>

<p>The Kubernetes API server validates and configures data for
the api objects which include pods, services, replicationcontrollers,
and others. The API Server services REST operations and
provides the frontend to the cluster's shared state through
which all other components interact.</p>

<p>The API is extensible. The core is <code>v1</code> .
An important add-on API is <code>apps/v1</code>,
which defines the <code>Deployment</code> and <code>ReplicaSet</code> objects.</p>

<h3>Cluster Management</h3>

<p><code>kubeadm</code></p>

<p>Install/Upgrade tools using zero-downtime method</p>

<pre><code class="language-bash"># Install kubeadm
sudo apt-get install -y --allow-change-held-packages \
    kubeadm=1.21.1-00
kubeadm version

# Inform the node
sudo kubeadm upgrade node

# Upgrade kubectl and kubelet to match kubeadm version
sudo apt-get install -y --allow-change-held-packages \
    kubectl=1.21.1-00 \
    kubelet=1.21.1-00

# Restart kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes
</code></pre>

<h2>CNI : Container Network Interface</h2>

<p>The Pod network is created and managed by a CNI-compliant provider; Calico, Cilium, &hellip;</p>

<h4><a href="https://chatgpt.com/c/f11fe422-3170-4f7c-ac35-79cfee471b3a">CNI traffic</a></h4>

<p>The CNI harnesses Linux Network Namespaces to dynamically create a virtual interface (e.g., <code>veth*</code>) per Pod. All Pod-to-Pod (Container-to-Container in effect) and Pod-to-Service intra-node traffic AKA <strong>east-west traffic</strong> is via these virtual adapters. To handle inter-node AKA cross-node traffic, an encapsulation scheme is implemented; IP-in-IP or VXLAN. Encapsulation provides for tunneling (L2 over L3) from Pod to Host nework. that is, allowing the otherwise-isolated traffic on the virtual adapter (<code>veth*</code>) to be routed through the node network (<code>eth*</code>).</p>

<p>BGP is a more direct (lower overhead) alternative to encapsulation schemes such as IP-in-IP and VXLAN.</p>

<pre><code class="language-bash"># Check for BGP status
calicoctl node status
</code></pre>

<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  logSeverityScreen: Info
  nodeToNodeMeshEnabled: true   # BGP mesh is enabled
</code></pre>

<h5>East-West Traffic</h5>

<ol>
<li><p><strong>Pod-to-Pod</strong> (Same Node)</p>

<ul>
<li>Handled by the virtual Ethernet (<code>*veth*</code>) adapter pairs created by Calico.
No network traffic leaves the node, and it's routed internally through the Linux kernel.</li>
</ul></li>

<li><p><strong>Pod-to-Pod</strong> (Different Nodes)</p>

<ul>
<li>Routed between the nodes vie physical adapter (<code>eth*</code>)
by one of several possible modes, per CNI configuration:

<ol>
<li>IP-in-IP (Encapsulation): Protocol 4. The traffic is encapsulated in an IP-in-IP packet
and sent over the network to the destination node via physical adapaters.</li>
<li>VXLAN (Encapsulation): Similar to IP-in-IP, but using VXLAN tunneling (L2 over L3).
This mode is often used in environments where BGP is not available.</li>
<li>BGP (Non-Encapsulation): If supported by host network (&quot;<code>nc -zv $peer_ip 179</code>&quot;),
the CNI configures nodes to exchange routes via BGP,
allowing direct routing of pod traffic without encapsulation.
This mode is the most efficient, as it has no encapsulate/decapsulate overhead.</li>
</ol></li>
</ul></li>

<li><p><strong>Pod-to-Service</strong></p>

<ul>
<li>Outgoing : Typically routed through a <code>ClusterIP</code> via node kernel
handling <strong>NAT</strong>/<strong>Masquerade</strong> using <code>iptables</code> or IPVS for routing
to the appropriate pod endpoint (&quot;<code>sudo iptables -t nat -L -n -v</code>&quot;).
If the service is of type <code>NodePort</code> or <code>LoadBalancer</code>,
the traffic might involve additional routing outside of the cluster,
potentially involving network load balancers.</li>
<li>Incomming : kube-proxy<br></li>
</ul></li>
</ol>

<p>Firewall considerations</p>

<pre><code class="language-bash"># IP-in-IP aka &quot;Protocol Number 4&quot; aka L3 (IPv4) Encapsulation
firewall-cmd --permanent --zone=$z --add-protocol=4

# VXLAN : L2 over L3 Encapsulation 
firewall-cmd --permanent --zone=$z --add-port=4789/udp

# BGP (Border Gateway Protocol)
firewall-cmd --permanent --zone=$z --add-port=179/tcp

# Masquerade (NAT)
firewall-cmd --permanent --zone=$z --add-masquerade

</code></pre>

<h2>K8s API Server Objects</h2>

<h3>Resources Available</h3>

<pre><code class="language-bash"># List all API resources (v1)
kubectl api-resources |less
</code></pre>

<pre><code class="language-text">NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
...
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
...
</code></pre>

<ul>
<li>Extensible API</li>
<li><code>apps/v1</code> is the most important add-on API.</li>
</ul>

<p>Have hierarchy:</p>

<ul>
<li><code>Deployment</code>: Represents tha deployed app.

<ul>
<li><code>ReplicaSet</code>: Manages app replicas.

<ul>
<li><code>Pods</code>: Adds features required to run the app.</li>
</ul></li>
</ul></li>
<li><code>ConfigMap</code></li>
<li><code>Secrets</code></li>
<li><code>PersistentVolumes</code></li>
</ul>

<h3>Object : <code>Node</code> (Minion)</h3>

<ul>
<li>A k8s minion server that runs a <code>kubelet</code>.</li>
<li>A compute unit.</li>
<li>May run on a multitue of platforms:

<ul>
<li>A server</li>
<li>An OS</li>
<li><code>systemd</code> (Linux system and service manager)</li>
<li><code>kubelet</code> (node agent) that runs on every node.</li>
<li>Container runtime (e.g., Docker engine)</li>
<li>Network proxy (<code>kube-proxy</code>) that handles K8s Services:

<ul>
<li>ClusterIP; internal Service that load balances k8s Pods</li>
<li>NodePort; open port on k8s node that load balances Pods</li>
<li>LoadBalancer; external to the cluster</li>
</ul></li>
<li>Container Network Interface (CNI) provider</li>
</ul></li>
</ul>

<h3>Object : <code>Pod</code></h3>

<ul>
<li>An abstraction of a server running an app.</li>
<li>Can run one or more containers with a single NameSpace,
exposed by a single IP address.</li>
<li>Typically created automatically by <code>Deployment</code>, or other API objects.</li>
<li>Regarding Docker image tags, do NOT use <code>latest</code> .</li>
<li>A collection of Kubernetes <strong><em>namespaces</em></strong> in a specific configuration; contains the following Linux namespaces (kernel filesystem component); provides the base functionality to create a running container from an image, and to scale and load balance per service within the software-defined networking (SDN) system spanning a K8s cluster.

<ul>
<li>PID namespaces</li>
<li>A single networking namespace</li>
<li>IPC namespace</li>
<li><code>cgroup</code> (control group) namespace.</li>
<li><code>mnt</code> (mount) namespace</li>
<li><code>user</code> (user ID) namespace</li>
</ul></li>
</ul>

<h3>Object : Higher-level Abstractons</h3>

<p>Typically spawn one or more Pods; typically create <strong><em>replica objects</em></strong>, which then create Pods.</p>

<h4><code>Deployment</code></h4>

<p>The most common object; deploys a mircroservice</p>

<h4><code>Job</code></h4>

<p>Run a Pod as a batch process</p>

<h4><code>StatefulSet</code></h4>

<p>Host applications requiring specific needs; oftten stateful, e.g., data store.</p>

<p>Features</p>

<ul>
<li>Ordinal Pod naming to ensure unique network identifiers</li>
<li>Persistent storage always mounted to same Pod</li>
<li>Ordered start/scale/update</li>
</ul>

<h4><code>DaemonSet</code></h4>

<p>Run a single Pod as an &quot;agent&quot; on every node in a cluster; system services, storage, logging, ...</p>

<h2>YAML Manifest</h2>

<h3>Ingredients (fields):</h3>

<ul>
<li><code>apiVersion:</code> K8s API-server version</li>
<li><code>kind:</code> K8s Object type</li>
<li><code>metadata:</code> K8s Object sdministrative information</li>
<li><code>spec:</code> K8s Object specifics; <code>container:</code>(s) <code>name:</code>, <code>image:</code>, <code>command:</code></li>
</ul>

<p>Get more info about the field</p>

<pre><code class="language-bash">kubectl explain $type.$fieldName[.$fieldName]
# E.g., 
kubectl explain pods.metadata
kubectl explain pods.spec.restartPolicy
# ALL possibilities
kubectl explain --recursive  pod.spec
</code></pre>

<ul>
<li>Use JSONPath identifer syntax</li>
</ul>

<h3>Generate YAML</h3>

<pre><code class="language-bash">kubectl create|run --dry-run=client -o yaml &gt; app.yaml
</code></pre>

<h3>&nbsp;</h3>

<!-- 

# Markdown Cheatsheet

[Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet "Wiki @ GitHub")


# Link @ (HTML | MD)

([HTML](___.md "___"))   


# Bookmark

- Reference
[Foo](#foo)

- Target
<a name="foo"></a>

-->
 
    </main>

    <script src="https://sempernow.github.io/refpages/sa/js/base.js"></script>
    <script>
        ;(function(o, undefined){
            'use strict'
            window.addEventListener('load', () => {
                ;(() => {})//()
                ;(() => {})//()
                ;(() => { // FOO LAB
                    const log = o.log('foo')
                        ,main = o.css('MAIN')
                    log('foo')
                    o.toDOM(main, '<h1>TEST</h1>')
                })//()
            })
        })( (typeof window !== 'undefined') 
            && (window[__APP__] = window[__APP__] || {})
                || (typeof global !== 'undefined') 
                    && (global[__APP__] = global[__APP__] || {})
        );
    </script>
</body>
</html>
